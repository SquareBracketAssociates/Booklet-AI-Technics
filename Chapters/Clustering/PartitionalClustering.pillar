!! Partitional Clustering
@sec:MLClustering-Basics

Real-world data is complex. To make sense of complex data requires approaches to shape it. While trying to make sense of it we can discover patterns to understand our data better. One of the well-known methods for such a goal is clustering. A cluster is a set of objects that share similar features, while clustering is called the technique that divides the observations (called ''data points'' in the Machine Learning vocabulary) into groups to get an insight into this data in an ""unsupervised way"". Clustering is also viewed as automatic labelling a dataset, by discovering the groups in it. You can use clustering methods, for example, to identify communities in social networks, group music for different themes, perform customer segmentation, implement a recommendation system, among many other use cases.

In a more object-oriented sense, a clustering algorithm is an object which can group data points and implements multiple features such as exclusiveness, nesting, completeness, and others such as metrics factors. We will discuss in the next sections its properties, although our main focus will be in how to apply clustering techniques in such a way that you could translate it into your experiments. We start by showing a high-level description of the API and then move to experiment with a toy dataset so you can get an overview of some available methods to play with. 

In the next chapter, you could read how to validate your own Machine Learning clusterings with the help of the MLMetrics package.

!!! API
@sec:MLClustering-APIIntroduction

The following section will describe a few properties of the clustering algorithms. They are not mandatory to learn or use if the goal is to build a model to make predictions, but they are a good basis for understanding which clustering technique to use in case it is not self-evident for your problem at hand. Sometimes multiple techniques could be tried before reaching a definitive conclusion. It is important to understand the process is highly-dependent of the context, and the properties of the dataset you want to cluster.

!!! Exclusiveness
@sec:MLClustering-Exclusiveness

A clustering algorithm could consider its observations to be:

- ""Exclusive"": Each observation can only belongs to one cluster. Answers <true> to #isExclusive.
- ""Overlapping"": An observation could be grouped in multiple clusters. Answer <true> to #isOverlapping.
- ""Fuzzy"": Maps each observation to its own cluster. Answer <true> to #isFuzzy.

To get which kind of exclusiveness implements the k-mean algorithm:

[[[
MLKMeans isExclusive.
]]]

!!!! Nesting
@sec:MLClustering-Nesting

Another distinguishing characteristic is when the algorithm allows clusters to be nested. These types of algorithms are said to implement ""Hierarchical Clustering"" (examples are the algorithms DBSCAN, OPTICS and Ward). On the other side, when nesting is not allowed during cluster formation, it is ""Partitional Clustering"" (algorithms k-means, k-medoids, k-medians, mean-shift, etc). We can obtain which type of nesting implements a class using the following methods:

- ""Hierachical Clustering"": Answer <true> to #isHierarchical.
- ""Partitional Clustering"": Answer <true> to #isPartitional.

For example to get which type of nesting implements the k-mean algorithm:

[[[
MLKMeans isPartitional.
]]]

!!!! Completeness
@sec:MLClustering-Completeness

It applies to observations which could be left without belonging to any cluster. To query which kind is implememented by a class, we have two methods:

- ""Complete"": All observations are assigned to a cluster. Answer ''<true>'' to #isComplete.
- ""Partial"": There are observations which could have no cluster. Answer ''<true>'' to #isPartial.

!! k-means Clustering
@sec:MLClustering-KMeansIntroduction

k-means is a clustering prediction method from an unlabeled dataset. It is very popular due to being fast, but as a drawback, it requires a number of clusters (k) to be specified beforehand. In this regard, an inappropriate choice for K may result in a poor clustering performance. 

Another requirement of k-means is that variables to be analysed must be normalized (standardized). The procedure to ensure the variables are normalized is to scale them so the mean is zero and standard deviation (the sum of the covariance) is one. Such standardized real numbers are also known as z-scores. Normalization is a common step while doing exploratory analysis, as part of a data wrangling stage prior to applying any machine learning algorithm.

!!! Algorithm Description
@sec:MLClustering-KMeansDescription

The k-means formulation builds ""non-overlapping"" groups by randomly selecting K prototypes from the input observations. These prototypes are selected by the algorithm itself as centroids to represent each cluster. We will work with now with a simplification to visualize how the iteration proceeds:

- In Fig. 1 initially we have points (observations) into a data space:

+Placing of data points>file://figures/KMeans-Stage1.png|width=50|label=figKMeans1+

%{{{
%\begin{figure}[H]
%\centering
%\includegraphics[width=\textwidth]{Chapters/Clustering/figures/KMeans-Stage1}
%\caption{k-means Stage 1}
%\label{figKMeans1}
%\end{figure}
%}}}


- Suppose we choose K = 2, so two random points into the data space are placed. We call them "centroids" ''1'' and ''2'', they can be seen In Fig. 2:


+Placing of centroids>file://figures/KMeans-Stage2.png|width=50|label=figKMeans2+

%{{{
%\begin{figure}[H]
%\centering
%\includegraphics[width=\textwidth]{Chapters/Clustering/figures/KMeans-Stage2}
%\caption{k-means Stage 2}
%\label{figKMeans2}
%\end{figure}
%}}}



- In Fig. 3, we assign each point to its nearest centroid, in our case, A B C are assigned to the centroid 1, and the D, E are assigned to the centroid 2.

+Assignment of data points>file://figures/KMeans-Stage3.png|width=50|label=figKMeans3+

%{{{
%\begin{figure}[H]
%\centering
%\includegraphics[width=\textwidth]{Chapters/Clustering/figures/KMeans-Stage3}
%\caption{k-means Stage 3}
%\label{figKMeans3}
%\end{figure}
%}}}


- The distance from all centroids to all data points is then calculated: Multiple distance algorithms are available, but it is common to use the Euclidean method. Others algorithms like the Manhattan distance can be used. The Euclidean distance is implemented in the method Point>>dist: in Pharo. Figure 4 shows that data points were assigned to its closest centroid.

+The key part of k-means algorithm is to "move" each centroid to the average location of its assigned observations. The move causes some of the initial assignments not to be closer to its centroid anymore, and then assignments should be updated.>file://figures/KMeans-Stage4.png|width=50|label=figKMeans4+

%{{{
%\begin{figure}[H]
%\centering
%\includegraphics[width=\textwidth]{Chapters/Clustering/figures/KMeans-Stage4}
%\caption{k-means Stage 4}
%\label{figKMeans4}
%\end{figure}
%}}}

- Updating cluster centers and reassigning observations continues till the cluster centers stop changing.

+Recalculation of distance of data points>file://figures/KMeans-Stage5.png|width=50|label=figKMeans5+

%{{{
%\begin{figure}[H]
%\centering
%\includegraphics[width=\textwidth]{Chapters/Clustering/figures/KMeans-Stage5}
%\caption{k-means Stage 5}
%\label{figKMeans5}
%\end{figure}
%}}}

- Iteration and refinement

+Iteration>file://figures/KMeans-Stage6.png|width=50|label=figKMeans6+

%{{{
%\begin{figure}[H]
%\centering
%\includegraphics[width=\textwidth]{Chapters/Clustering/figures/KMeans-Stage6}
%\caption{k-means Stage 6}
%\label{figKMeans6}
%\end{figure}
%}}}


!!!! The Iris Dataset
@sec:MLClustering-Iris

Our final goal in this section is to build a ""clustering model"" to make predictions. We will work with the famous ""Iris-Flowers"" plants dataset, published by the statistician and biologist Ronald Fisher in 1936. You will see it named as the Edgar Anderson's Iris Flowers, because it was the botanic who collected the flowers. It consists of 150 flowers, carefully measured, and divided into 3 species groups of 50 flowers each. We advise to not underestimate the features of this dataset, as it is used as a toy data example as a basis for understanding how clustering works. 

The Iris-Flowers dataset is available in the Datasets package which can be loaded with the expression:

[[[
Metacello new
  baseline: 'Datasets';
  repository: 'github://PharoAI/Datasets';
  load.
]]]

We can quickly inspect the dataset contents by evaluating:

[[[
Dataset loadIris.
]]]

We can observe that the dataset:

- Each row represents a flower.
- It has 3 species of flowers: ''Setosa'', ''Versicolor'', and ''Virginica''.
- It has 5 features: ''PetalLength'', ''PetalWidth'', ''SepalLength'', ''SepalWidth'', and ''Species''. The sepal and petal parts measures are expressed in centimeters and, as reference, they could be seen with the folllowing picture:

+Difference between Sepal and Petal features>file://figures/451px-Petal-sepal.png|width=40|label=figPetalSepal+

%{{{
%\begin{figure}[H]
%\centering
%\includegraphics[width=\textwidth]{Chapters/Clustering/figures/451px-Petal-sepal}
%\caption{Difference between Sepal and Petal features}
%\label{figPetalSepal}
%\end{figure}
%}}}

We can query here the first observations to get a overview of the dataset:

[[[
Datasets loadIris head.
]]]

+Inspector of the first flowers in the Iris Dataset>file://figures/Iris_DataFrame_1.png|width=100|label=figDFIrisInspect+

%{{{
%\begin{figure}[H]
%\centering
%\includegraphics[width=\textwidth]{Chapters/Clustering/figures/Iris_DataFrame_1}
%\caption{Inspector of the first flowers in the Iris Dataset}
%\label{figDFIrisInspect}
%\end{figure}
%}}}

By quickly checking the data, we can see we are conceding multiple assumptions here: The whole dataset comes from a single experiment under almost perfect laboratory conditions, where variables are in the same measure (in this case centimetres), hence comparable, and even published in a peer-reviewed scientific journal! Real scenarios are not so kind. Variables could have different types and importance, it could contain missing values everywhere, have an undefined or unparseable format, probably should be combined, scaled and scored, just to name a few data preprocessing steps.

For now, we can get the first challenge here, namely interpretation. Which variables are the important ones in this dataset to get a predictive model? All of them? It would be nice to plot these observations with the Roassal visualization engine, but we have four real-valued variables and just only 2 or 3 dimensions we can "easily" plot. We will see two dimensions in the next example. In this case, a good idea is to drop the labels column and set all data points with the same colour, so we can get a feel of a real-life dataset. Let's take the y dimension to be the "SepalLength" and the x dimension to be "SepalWidth":

[[[
MLR2ScatterPlotViz plot: Datasets loadIris.
]]]

+Initial visualization of two Iris Dataset features (without cluster predictions)>file://figures/MLScatterPlotViz_1.png|width=100|label=figMLIrisViz1+

%{{{
%\begin{figure}[H]
%\centering
%\includegraphics[width=\textwidth]{Chapters/Clustering/figures/MLScatterPlotViz_1}
%\caption{Inspector of the first flowers in the Iris Dataset}
%\label{figMLIrisViz1}
%\end{figure}
%}}}

!!!! Simple Example
@sec:MLClustering-KMeansSimpleExample

Create a clustering using:

[[[
| df kmeans |
df := Datasets loadIris.
kmeans := KMeans numberOfClusters: 3.
]]]

The selection of a "good value" of K usually requires a good understanding of the domain model, for example, one may choose K = 3 under the knowledge of the domain model has three kinds of diagnostics (whatever they are), or K = 2 when there are two types of insurances, players, or any other object. You can easily run KMeans with several values of K, but at some point, the refinement makes no difference to the final predictions and comparing results between different values of K is time-consuming.

Of course, we already know there are three real clusters in the Iris-Flowers dataset but let us pretend, for the sake of learning k-means, to ignore the "species" column which contains the three clusters. We will see later there are methods to guess a good number of clusters, for example, the "elbow method".

So far we have a k-means object, which does nothing but gets instantiated. To start building the k-means cluster model, it should be "fitted" or " trained". To build the model, we select the features we would like to use to predict our target variable.

[[[
kmeans fit: df.
]]] 


We can also setup the maximum number of iterations to take when the centroids do not coincide:

[[[
kmeans maxIterations: 20.
]]]


Now that we predicted a value for K, let's get back to the Roassal visualization, this time we configure a value of K:

[[[
MLR2ScatterPlotViz plot: Datasets loadIris.
]]]


+Plot Petal Length versus Petal Width features>file://figures/petal_length-vs-petal_width.png|width=95|label=figPlotIris+

%{{{
%\begin{figure}[H]
%\centering
%\includegraphics[width=\textwidth]{Chapters/Clustering/figures/petal_length-vs-petal_width}
%\caption{Plot Petal Length versus Petal Width in Iris Dataset}
%\label{figPlotIris}
%\end{figure}
%}}}

We can see that the first two features, "Sepal Length" versus "Sepal Width", does not seem to be good predictors for cluster recognition. We could try combinations of different features by changing ''featureASelector'' and ''featureBSelector'' to select other features (#second versus #third, #third versys #fourth, etc.) however there is a useful plot for these cases, the scatter plot matrix:

[[[
MLR2ScatterMatrixViz plot: Datasets loadIris.
]]]

+Scatter matrix of Iris features>file://figures/scatterplot-iris.png|width=90|label=figScatterIris+

%{{{
%\begin{figure}[H]
%\centering
%\includegraphics[width=\textwidth]{Chapters/Clustering/figures/scatterplot-iris}
%\caption{Plot Petal Length versus Petal Width in Iris Dataset}
%\label{figScatterIris}
%\end{figure}
%}}}


If we want to get a scatter plot of two features for some K, and setting colors of the data points, we could do it this way:

[[[
MLR2ScatterPlotViz new
	k: 3;	
	featureASelector: #second;
	featureBSelector: #third;
	colors: { 
		'virginica' 	-> Color blue .
		'versicolor'	-> Color green .
		'setosa' 		-> Color red } asDictionary;
	initializeWithDataFrame: Datasets loadIris;
	plot.
]]]


How to use the prediction model?

''Work in progress''

!!! Variations of k-means
@sec:MLClustering-KMeansVariations

We have seen what is known as the LLoyd's algorithm for k-means, published in 1982. The other variants are:

- Gonzalez algorithm (fork-center). This biases too much to outlier points.
- Hartigan-Wong which is often the fastest.
- A recent algorithm (from Arthur and Vassilvitskii) called k-means++

''Work in progress''

!!! Evaluating clustering performance

Every clustering algorithm has its own advantages and drawbacks. Validation of cluster analysis is an important aspect of measuring how well multiple clustering methods performed on a data set. As clustering is more about discovering than an exact prediction method, several indices can be used to determine if the right number of clusters were discovered. This is what the K in clustering algorithms (k-means, k-medians, kNN) is about after all.

Roughly, there are two major strategies to assess clustering quality:

""External cluster evaluation"": Which involves comparing results of a clustering method with already known labels of another reference cluster. This type of assessment is similar to supervised learning, in the sense that at least one of the clustering results is considered to have the ground truth clusters or represents the optimal solution. Examples of this type of metric are the ''Rand Index'' (Rand, 1971), the ''Adjusted Rand Index'' or ''ARI'' (Hubert and Arabie, 1985), the ''F-measure'', the ''Jaccard Index'' (Anderberg, 1973), the ''Dice Index'' (Dice, 1945; Sørensen, 1948) and the ''Normalised Mutual Information'' or ''NMI''.

""Internal cluster evaluation"": Which does not use information outside the dataset, like human annotations, and involves summarization into a single quality score. In many applications ground truth labels are not available (consider for example streaming applications). Examples of this type of metric are the ''Root-mean-square standard deviation'' or ''RMSSTD/RMSD/RMSE'' and the ''R-squared'' or ''RS'' (Sharma, 1996), the ''Silhouette Coefficient'' (Rousseeuw, 1987), the ''Davies-Bouldin Index'' or ''DB'' (Theodoridis and Koutroubas, 1999).

!!!! Installation
@chap:MLMetrics-Installation

All algorithms described in this chapter are part of the MLMetrics project which can be found at *https://github.com/pharo-ai/MLMetrics*. To install MLMetrics, go to the Playground in your Pharo image and execute the following Metacello request (select it and press Do-it button or Ctrl+D):

[[[
Metacello new
  baseline: 'MLMetrics';
  repository: 'github://pharo-ai/MLMetrics/src';
  load.
]]]

!!!! API Definition
@sec:MLMetrics-APIDefinition

We will introduce here general conventions and practices understood by all metrics. These methods are considered stable and supported for all metrics implemented in the ==MLMetrics== package. 

Most, if not all, metrics are known with different names. All the metrics subclass ==MLClusteringMetric== and implements the #names method answering a ==Collection== of sub-collections, where each one contains the synonyms for each metric in English language. You can obtain all the available metrics names with ==availableMetricNames== as follows:

[[[
	MLClusteringMetric availableMetricNames.
	"#(
	#('Rand Index' 'RI' 'General Rand Index') 
	#('Adjusted Rand Index' 'HA' 'ARI') 
	#('Jaccard Index' 'Intersection over union' 'Tanimoto coefficient' 'IOU') 
	#('Fowlkes Mallows index' 'FMI' 'FM' 'Gmean' 'G-mean') 
	#('Mirkin Index') 
	#('Rand Index' 'RI' 'General Rand Index')
	)"
]]]

Also all metrics return a ==Float== value. In this case, an utility method to configure the number of wished decimals is provided for any metric:

[[[
	MLAdjustedRandIndex new numberOfWishedDecimal: 4.
]]]

To configure the default number of wished decimal for any metric, you may override the ==defaultNumberOfWishedDecimal== method in your own class.

The method to obtain the ""resulting metric"" for any metric implementation is ==computeMetric==. This method always returns a ==Float==, even for cases where a formula has no defined behavior. In this case, we explicitly state the situation in the method comment.

All clustering metrics implements ==clusterA== and ==clusterB== accessors (getters and setters) with collections as inputs to be compared. It is not enforced clusterA to represent the ''ground truth'', however it is common practice to configure the collection with "truth labels" as the first parameter, and the second collection to be interpreted as the collection with predictions (predictions are the result of your clustering algorithm).

!!!! Rand Index
@sec:MLMetrics-RandIndex

A "Rand Index" (RI) is a Float between 0.0 and 1.0 representing the number of agreements between two data clusterings, presumably from two different methods on the same data set. You may use the Rand Index score to compare how two partitions (clustering results) agreed. A value of 1.0 means that clusterings between two methods were the same, and these partitions agree perfectly, and 0.0 means that the two data clusterings disagree on any pair of points.

!!!!! Formal Definition
@sec:MLMetrics-Definition

Let {{{$C_1$}}} and {{{$C_2$}}} be two sets, the ''Rand Index'' (R) is defined by the formula:

{{{
\[ R(C_1,C_2) = \frac{a+b}{a+b+c+d} = \frac{a+b}{{n \choose 2 }} \]
}}}

where:

- {{{$a$}}} is the number of pairs of elements that are in the same subset in both sets {{{$C_1$}}} and {{{$C_2$}}}.
- {{{$b$}}} is the number of pairs of elements that are in different subsets in both sets {{{$C_1$}}} and {{{$C_2$}}}.
- {{{$c$}}} is the number of pairs of elements that are in the same subset in {{{$C_1$}}} but in different ones in {{{$C_2$}}}
- {{{$d$}}} is the number of pairs of elements that are in different subsets in {{{$C_1$}}} but in the same in {{{$C_2$}}}
- {{{$a + b$}}} the number of correct similar pairs plus the number of correct dissimilar pairs.
- {{{$n \choose 2 $}}} is the number of total possible pairs (without ordering).

If one of the partitions is considered to have the ground truth labels, the index can be also analysed as counting the ''True Positives'' (TP), ''True Negatives'' (TN), ''False Positives'' (FP) and ''False Negatives'' (FN) with the following equivalent formula:

{{{
\[ RI = \frac {TP + TN}{TP + FP + FN + TN} \]
}}}

In such case, it is common to refer the first set to have "true clusterings" and the second set to represent the "predicted clusterings". The interpretation of terms under such view of the index is:

|TP | Pairs of elements which belongs to class positive and we clustered it as positive
|FP | Pairs of elements which belongs to class negative and we clustered it as positive
|FN | Pairs of elements which belongs to class positive and we clustered it as negative
|TN | Pairs of elements which belongs to class negative and we clustered it as negative

!!!!! Algorithm Description
@sec:MLMetrics-RIAlgorithmDescription

The algorithm is based on counting pair of elements from both sets. It builds every posible unordered pair of elements in each cluster result. Which pairs? The answer is the binomial coefficient and you can easily print this combination for any Collection by evaluating:

[[[
Transcript open.
#(A B C)
	combinations: 2 
	atATimeDo: [ :each | each traceCr ].
]]]

Then it counts the number of pairs, ""in term of indices"", which are in both clustering methods, and the number of pairs which are in different clusters. We will see this in detail in a following partial agreement example.

!!!!! Simple Example
@sec:MLMetrics-RISimpleExample

We begin considering the most basic cases, to get an overview of the API. We can obtain the Rand index in Pharo using the ==MLRandIndex== class. In this case we will be passing two clusterings (Collections) of 5 assignments each one. Notice both input collections must have the same number of assignments, and each one could take a value of 1, 2 or 3. Conceptually, each assignment represents a pair, but this will be more clear in a follow-up example. For now let's check two exact clusterings (both sets has the same assignments):

[[[
MLRandIndex 
    clusterA: #(1 1 2 3 3) 
    clusterB: #(1 1 2 3 3).
]]]

The result in the Rand Index is 1.0. If we consider two partitions without any pair in common:

[[[
MLRandIndex 
    clusterA: #(0 0 0 0 0 0) 
    clusterB: #(0 1 2 3 4 5).
]]]

Results in 0.0, as every pair of points are in different clusters. The index is also symmetric:

[[[
MLRandIndex 
    clusterA: #(0 1 2 3 4 5)
    clusterB: #(0 0 0 0 0 0).
]]]

yields the same result as above of 0.0.

!!!!! Partial Agreement Example
@sec:MLMetrics-RIUserExample

Imagine your application has queried five famous philosophers. One clustering method (such as k-means, k-medians, DBSCAN, or other) discovered three clusters (say 1, 2 and 3), and these are stored in {{{$clusterResult1$}}}. Another method discovered also three clusters, storing them to {{{$clusterResult2$}}} respectively. Notice that, in common tasks, one of these clusterings may be considered the reference cluster, for example, if one of them has a column which assigned the true labels. 

We may represent the clustering as follows:

[[[
| clusterResult1 clusterResult2 |

clusterResult1 := { 
	'Spinoza'  -> 1 . 'Bentham' -> 1 . 
	'Kant'     -> 2 . 
	'Foucault' -> 3 . 'Plato'   -> 3 } collect: #value.

clusterResult2 := { 
	'Spinoza'  -> 1 . 'Bentham'  -> 1 . 
	'Kant'     -> 2 . 'Foucault' -> 2 .
	'Plato'    -> 3 } collect: #value.
]]]

Our collection has 5 elements, which results in 10 possible pairs of points for each clustering: 

{{{ \[ pairsClusterResult1 \Rightarrow \{ 
\[ \{ Pair \{1, 2\} = (Spinoza, Bentham) = (Cluster 1, Cluster 1) \} \]
\[ \{ Pair \{1, 3\} = (Spinoza, Kant) = (Cluster 1, Cluster 2)  \} \]
\[ \{ Pair \{1, 4\} = (Spinoza, Foucault) = (Cluster 1, Cluster 3)  \} \]
\[ \{ Pair \{1, 5\} = {Spinoza, Plato) = (Cluster 1, Cluster 3) \} \]
\[ \{ Pair \{2, 3\} = (Bentham, Kant) = (Cluster 1, Cluster 2) \} \]
\[ \{ Pair \{2, 4\} = (Bentham, Foucault) = (Cluster 1, Cluster 3) \} \]
\[ \{ Pair \{2, 5\} = (Bentham, Plato) = (Cluster 1, Cluster 3) \} \]
\[ \{ Pair \{3, 4\} = (Kant, Foucault) = (Cluster 2, Cluster 3) \} \]
\[ \{ Pair \{3, 5\} = (Kant, Plato) = (Cluster 2, Cluster 3) \} \]
\[ \{ Pair \{4, 5\} = (Foucault, Plato) = (Cluster 3, Cluster 3) \} \]

\[ \{ pairsClusterResult2 \Rightarrow \{ \]
\[ \{ Pair \{1, 2\} = (Spinoza, Bentham) = (Cluster 1, Cluster 1) \} \]
\[ \{ Pair \{1, 3\} = (Spinoza, Kant) = (Cluster 1, Cluster 2) \} \]
\[ \{ Pair \{1, 4\} = (Spinoza, Foucault) = (Cluster 1, Cluster 2) \} \]
\[ \{ Pair \{1, 5\} = (Spinoza, Plato) = {Cluster 1, Cluster 3) \} \]
\[ \{ Pair \{2, 3\} = (Bentham, Kant) = (Cluster 1, Cluster 2) \} \]
\[ \{ Pair \{2, 4\} = (Bentham, Foucault) = (Cluster 1, Cluster 2) \} \]
\[ \{ Pair \{2, 5\} = (Bentham, Plato) = (Cluster 1, Cluster 3) \} \]
\[ \{ Pair \{3, 4\} = (Kant, Foucault) = (Cluster 2, Cluster 2  \} \]
\[ \{ Pair \{3, 5\} = (Kant, Plato) = (Cluster 2, Cluster 3) \} \]
\[ \{ Pair \{4, 5\} = (Foucault, Plato) = (Cluster 2, Cluster 3) \} \]
}}}

Now let's proceed to count pairs of indices, suming up 1 as each case appears according to the four cases in the Rand Index formula. Our initial state for the four cases is a = 0, b = 0, c = 0 and d = 0. To follow the next iterations, recall the formula we previously defined in the Formal Definition section:

- Pair {1, 2} in {{{$pairsClusterResult1$}}} and {{{$pairsClusterResult2$}}} both have (1,1) -> they are assigned to the same cluster in {{{$pairsClusterResult1$}}} and {{{$pairsClusterResult2$}}}. {{{$a$}}} is incremented {{{$a$ = 1}}}
- Pair {1, 3} in {{{$pairsClusterResult1$}}} and {{{$pairsClusterResult2$}}} both have (1,2) -> they are assigned to different cluster in {{{$pairsClusterResult1$}}} and different cluster in {{{$pairsClusterResult2$}}}, {{{$b$}}} is incremented (b = 1)
- Pair {1, 4} in {{{$pairsClusterResult1$}}} is (1,3) and in {{{$pairsClusterResult2$}}} is (1,2) -> they are assigned to different cluster in {{{$pairsClusterResult1$}}} and different cluster in {{{$pairsClusterResult2$}}}, {{{$b$}}} is incremented (b = 2)
- Pair {1, 5} in {{{$pairsClusterResult1$}}} and {{{$pairsClusterResult2$}}} both have (1,3) -> they are assigned to different cluster in {{{$pairsClusterResult1$}}} and different cluster in {{{$pairsClusterResult2$}}}, {{{$b$}}} is incremented (b = 3)
- Pair {2, 3} in {{{$pairsClusterResult1$}}} and {{{$pairsClusterResult2$}}} both have (1,2) -> they are assigned to different cluster in {{{$pairsClusterResult1$}}} and different cluster in {{{$pairsClusterResult2$}}}, {{{$b$}}} is incremented (b = 4)
- Pair {2, 4} in {{{$pairsClusterResult1$}}} is (1,3) and in {{{$pairsClusterResult2$}}} is (1,2) -> they are assigned to different cluster in {{{$pairsClusterResult1$}}} and different cluster in {{{$pairsClusterResult2$}}}, {{{$b$}}} is incremented (b = 5)
- Pair {2, 5} in {{{$pairsClusterResult1$}}} and {{{$pairsClusterResult2$}}} both have (1,3) -> they are assigned to different cluster in {{{$pairsClusterResult1$}}} and different cluster in {{{$pairsClusterResult2$}}}, {{{$b$}}} is incremented (b = 6)
- Pair {3, 4} in {{{$pairsClusterResult1$}}} is (2,3) and in {{{$pairsClusterResult2$}}} is (2,2) -> they are assigned to different cluster in {{{$pairsClusterResult1$}}} and the same cluster in {{{$pairsClusterResult2$}}}, {{{$d$}}} is incremented (d = 1)
- Pair {3, 5} in {{{$pairsClusterResult1$}}} and {{{$pairsClusterResult2$}}} both have (2,3) -> they are assigned to different cluster in {{{$pairsClusterResult1$}}} and different cluster in {{{$pairsClusterResult2$}}}, {{{$b$}}} is incremented (b = 7)
- Pair {4, 5} in {{{$pairsClusterResult1$}}} is (3,3) and in Y is (2,3) -> they are assigned to ths same cluster in {{{$pairsClusterResult1$}}} and different cluster in Y, {{{$c$}}} is incremented (c = 1)

We replace a, b, c and d with the values into the RI formula:

{{{
\[ R(C_1,C_2) = \frac{1+7}{1+7+1+1} = \frac{1+7}{{5 \choose 2 }} = 0.8 \]
}}}

You can confirm the result by evaluating:

[[[
MLRandIndex 
    clusterA: #(1 1 2 3 3) " clusterResult1 "
    clusterB: #(1 1 2 2 3). " clusterResult2 "
]]]

The Rand Index results is "0.8", and it represents the fraction of all pairs of points on which the two clusterings agree. 

In practice, the RI do not use the full range of possible values, and most of them lies between [0.5, 1] concentrating near the extremes. The problem is the classic RI is highly dependent upon the number of clusters: When the data set used is small or the number of clusters increases, there is a higher chance of agreements are overlapped just due to chance. Most applications fix this by applying one or several corrected versions of the RI. A variation of RI should enable to adjust the amount of agreement in the cluster solutions with chance normalization, this means measuring ""proportions of agreements"" between the two partitions, and ignoring permutations, meaning that renaming labels does not affecting the score.

This is what we will see in the next section.

!!!! Adjusted Rand Index
@sec:MLMetrics-ArjustedRandIndex

The "Adjusted Rand Index" (ARI) is a Float between -1.0 and 1.0, where positive values mean that pairs in the known clusters and predicted clusters are similar, being 1.0 the perfect agreement and 0.0 a chance agreement, and negative values mean that pairs in the known clusters and predicted clusters are highly different. In short, the higher the value of ARI, the better the predictive ability of the evaluated clustering method. The "adjusted" part comes from the fact that a random result is scored as 0.

A word of caution: There are several measures of ARI, and actually the first two initial implementations are used in the literature with similar names: The ''Morey and Agresti'' (1984) is usually referred as MA or {{{$ARI_m_a$}}}, and a corrected version from ''Hubert and Arabie'' (1985) known as HA or {{{$ARI_h_a$}}}. Variations of these ARI were further developed, among the most popular ones we may find the ''Fowlkes–Mallows Index'' or FMI and the ''Mirkin Metric'' (Mirkin, 1996). 

In this section we will describe first the {{{$ARI_h_a$}}} formula.

!!!!! Formal Definition
@sec:MLMetrics-ARIDefinition

The {{{$ARI_h_a$}}} is defined by the equation:

{{{
\[	ARI_h_a = \dfrac{\sum_{i,j}{n_{ij} \choose 2} - \sum_{i}{n_{i.} \choose 2}\sum_{j}{n_{.j} \choose 2} / {n \choose 2}}{\frac{1}{2}[\sum_{i}{n_{i.} \choose 2}+\sum_{j}{n_{.j} \choose 2}]-\sum_{i}{n_{i.} \choose 2}\sum_{j}{n_{.j} \choose 2} / {n \choose 2}} \]	
}}}

This form is commonly used to explain the details of the distribution which models the randomness (called the hyper-geometric distribution). Also it allows to describe the typical implementation of the algorithm, which uses a contingency table. 

- {{{$n_ij$}}} is the diagonal sums (when i = j).
- {{{$a_i$}}} is the row sums.
- {{{$b_j$}}} is the column sums.

Another notation for the formula is often expressed with the following representation of terms:

{{{ 
\[ ARI_h_a = \frac{\text{RI} - E[\text{RI}]}{\max(\text{RI}) - E[\text{RI}]} \]
}}}

where:

- The numerator RI is the (General) Rand Index.
- The denominator max(RI) represents the maximum possible RI, defined as the permutation sum of rows plus sum of columns divided then by 2: {{{ \[ Max(RI)=\frac{\sum{a_i\choose2}+\sum{b_j\choose2}}{2} \] }}}
- E(RI) is the Expected Maximum Index, which is used to obtain a correction to the Rand Index: {{{ \[ E(RI)=\sum_{i}{n_{i.} \choose 2}\sum_{j}{n_{.j} \choose 2} / {n \choose 2} \] }}}

!!!!! Algorithm Description
@sec:MLMetrics-ARIAlgorithmDescription

The algorithm constructs a contingency table (also known as a cross tabulation) by counting the co-occurrences of both cluster results. In general terms, a contingency table is used to describe data which has more than one categorical variable. The rows represents categorical variables with the truth values, say from {{{$clusterResult1$}}}, and columns represents values from predictions, for example in {{{$clusterResult2$}}}. This kind of matrix usually includes an extra row and a right-most column representing the ""marginals"", used to asses statistical significance. Each cell in the matrix, i.e. each intersection, is the number of times an element appears in the combination of the particular row and column intersected. The right-bottom cell represents the total number of elements involved and is called the ""grand total"". 

The contingency table for our example with philosophers is:

{{{
\[ \mathbf{CTAB}=\begin{bmatrix}2 & 0 & 0\\
0 & 1 & 0\\
0 & 1 & 1\\
\end{bmatrix} \]
}}}

A decomposition of the equation terms with substitution of variables:

{{{
\[ \sum_{ij} \binom{n_{ij}}{2}  = \binom{2}{2} + \binom{1}{2} + \binom{1}{2} = 4 \]
}}}

{{{
\[ \sum_i \binom{a_j}{2} = \binom{2}{2} + \binom{1}{2} + \binom{1}{2} = 4 \]
}}}

{{{
\[ \sum_j \binom{b_j}{2} = \binom{2}{2} + \binom{2}{2} + \binom{1}{2} = 5 \]
}}}

Therefore

{{{
\[ ARI = \frac{1 - 2 * 2 / 10}{(2 + 2)/2 - 2 * 2 / 10} =  0.375 \]
}}}

Which can be verified with:

[[[
MLAdjustedRandIndex 
    clusterA: #(1 1 2 3 3) " clusterResult1 "
    clusterB: #(1 1 2 2 3). " clusterResult2 "
]]]

We can observe 0.375 is much lower than the Rand Index result of 0.8, which is expected and a common situation, considering the Adjusted Rand Index can result in negative values for very dissimilar clusterings. To follow a standard interpretation of the result, Steinley (2004) indicates the following levels of agreement:

- An index greater than 0.90 are considered excellent recovery
- An index greater than 0.80 are considered good recovery 
- An index greater than 0.65 are considered moderate recovery 
- An index less than 0.65 are considered poor recovery

!!!! Fowlkes–Mallows Index
@sec:MLMetrics-FMI

The ''Fowlkes–Mallows Index'', also known as G-measure or FM, is a Float value between 0.0 and 1.0. It is an external evaluation method which is the geometric mean of precision and recall between two clusterings. The index can be used either for comparing flat and hierarchical clusterings.

!!!!! Formal Definition
@sec:MLMetrics-FMIDefinition

The FM index is commonly defined using information extraction terms:

{{{
\[ \text{FMI} = \frac{\text{TP}}{\sqrt{(\text{TP} + \text{FP}) (\text{TP} + \text{FN})}} \]
}}}

You may obtain the index in Pharo with the following expression:

[[[
	MLFowlkesMallowsIndex
		clusterA: #(1 1 2 3 3) " clusterResult1 "
		clusterB: #(1 1 2 2 3) " clusterResult2 "
]]]

!!!! Jaccard Coefficient
@sec:MLMetrics-Jaccard

The Jaccard Index, also known as the Jaccard similarity coefficient, is a Float value between 0.0 and 1.0. It was originally designed as a general similarity measure between two non-empty sets, but it can be used as an evaluation measure of the degree of overlap between vectors. Intuitively, it can be thought as the size of the intersection divided by the size of the union. As usual, the closer to 1.0, the more similar the two datasets, the closer to 0.0 the more dissimilar. This metric should not be confused with the ""Jaccard distance"", which gives a ""dissimilarity"" measure.

This metric is also known as "IoU" or Intersection over Union, specially in image detection contexts. 

!!!!! Formal Definition
@sec:MLMetrics-JaccardDefinition

The formal definition for the set-based version is:

{{{
\[ J(A,B) = {{|A \cap B|}\over{|A \cup B|}} = {{|A \cap B|}\over{|A| + |B| - |A \cap B|}} \]	
}}}

and it could appears also as:

{{{
\[ J = TP / (TP + FP + FN) \]
}}}

This formula is mostly used for binary classification tasks, i.e. elements in each input set are present or absent.

We will see below there is generalized version of the original formula. The formal definition for This version is:

{{{
\[ J_\mathcal{W}(\mathbf{x}, \mathbf{y}) = \frac{\sum_i \min(x_i, y_i)}{\sum_i \max(x_i, y_i)} \]
}}}

!!!!! Algorithm Description
@sec:MLMetrics-JaccardDescription

It is important to remark there are actually two versions of the Jaccard similarity index: A set-based version, and a vector-based (weighted) version. Both formulas are equivalent, but the vector-based version is just another method which was later implemented as a generalized version of the set-based one. The set-based version accept as input ''instance sets'' such as flat Collections of different sizes, while the vector-based version has the requirement of both input collections to have the same size. Also, under the weighted/vector-based variation, the elements of the union of both collections are considered as features, and a binary-vector is built where 0 means absence and 1 means presence of a feature, for both the numerator (with the minimum 1 or 0 at each feature) and denominator (with the maximum 1 or 0 at each feature). Finally both numerator and denominator are summed.

To better understand the difference between the set-based and vector-based versions, let's consider the set version first with inputs:

[[[
	cluster1 := { 'Plato' . 'Foucault' . 'Kant' }.
	cluster2 := { 'Plato' . 'Foucault' . 'Spinoza' . 'Bentham' }.
]]]

We should note here we do not have the cluster assignment information for any philosopher, but it is important to understand why it could be easier to apply the set-based version formula:

{{{
\[ {|cluster1 \cap cluster2|} = \{ Plato, Foucault, Kant \} = 3 \]
\[ {|cluster1 \cup cluster2|} = \{ Plato, Foucault, Kant, Spinoza, Bentham \} = 5 \]
\[ J(cluster1,cluster2) = {3 \over 5 } = 0.6 \]
}}}

Under the vector-based version, we "binarize" the features marking for presence/absence at each element position of the collection. And the sum of each vector divided give us the same result as expected:

[[[
	cluster1 := { 1 . 1 . 1 . 0 . 0 }.
	cluster2 := { 1 . 1 . 1 . 1  .1 }.
	(cluster1 sum / cluster2 sum) asFloat. "0.6"
]]]

!!!!! Simple Example
@sec:MLMetrics-JaccardSimpleExample

It is clear now that if we have assignment information for each philosopher, we should use the weighted Jaccard version. In such case, the ==MLWeightedJaccardIndex== explicitly uses the vector-based version:

[[[
	MLWeightedJaccardIndex
		clusterA: #(0 0 0 0)
		clusterB: #(0 1 2 3) " 0.25 "
]]]

If we interpret clusterA and clusterB as instance sets (flat collections), we can obtain the Jaccard coefficient with the following expression:

[[[
	MLJaccardIndex
		clusterA: #(0 0 0 0)
		clusterB: #(0 1 2 3) " 0.25 "
]]]

As can be seen, both versions are equivalent and it is the interpretation of the input data what could change.


!!!! Mirkin Index
@sec:MLMetrics-Mirkin

The Mirkin Index is an adjusted variation of the Rand Index, 




!!!! Normalised Mutual Information (NMI)
@sec:MLMetrics-NMI

The NMI is a Float number between 0.0 and 1.0 and represents a measure of 
NMI is a specialization of another metric called Mutual Information (MI), taking into account , and similarly to the RI, both NMI and MI are not adjusted against chance.

!!!!! Formal Definition
@sec:MLMetrics-NMIDefinition

{{{ 
\[ \text{NMI}(U, V) = \frac{\text{MI}(U, V)}{\text{mean}(H(U), H(V))} \]
}}}

!!!! Silhouette Coeffecient
@chap:MLMetrics-Silhouette

The Silhouette Coeffecient (also known as the Silhouette Index) is a Float value between -1.0 and 1.0 and represents a measure of how well observations are separated from neighboring clusters. A high value means that the object is well suited to its own cluster, and poorly suited to the adjacent cluster. It is useful when ground-truth labels are not known, which makes this metric an intrinsic evaluation type. Advantages of the Silhouette Index are that can used with any clustering algorithm, and may provide a concise graphical representation of how well each object has been categorized.

!!!!! Formal Definition
@sec:MLMetrics-SilhouetteDefinition

The Silhouette Coeffecient, for a single sample, is defined by:

{{{
\[ s = \frac{b - a}{max(a, b)} \]
}}}

where:

- {{{$a$}}} (also called the ""cluster cohesion"") represents the average intra-cluster distance, i.e. the distance between a data point and all other points in the same cluster.
- {{{$b$}}} (also called the ""cluster separation"") represents the average inter-cluster distance, i.e. the distance between a data point and all other points in the cluster nearest to the data point's cluster.

!!!!! Algorithm Description
@sec:MLMetrics-SilhouetteDescription

The method is based on the average distance from one given object to those of the same cluster as that object, compared with the similar average distance from the best alternative cluster. Roughly, the steps involved in the algorithm are:

- Calculate the average distance within the cluster.
- Each cluster member has its own average distance from all other members of the same cluster.
- The average of these averages is the dissimilarity score for the cluster.

'''Work in Progress'


!!!! Bibliography

Davies, D., Bouldin, D.: A cluster separation measure. IEEE PAMI 1(2), 224–227 (1979)

Dice, Lee R. (1945). "Measures of the Amount of Ecologic Association Between Species". Ecology. 26 (3): 297–302. doi:10.2307/1932409. JSTOR 1932409.

R. A. Fisher (1936). "The use of multiple measurements in taxonomic problems". Annals of Eugenics. 7 (2): 179–188. doi:10.1111/j.1469-1809.1936.tb02137.x. hdl:2440/15227

Fowlkes, E. B.; Mallows, C. L. (1 September 1983). "A Method for Comparing Two Hierarchical Clusterings". Journal of the American Statistical Association 78 (383): 553.

Hubert, L. and Arabie, P. 1985. Comparing partitions. Journal of Classification. 2: 193--218.

Rand, W.M. 1971. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association 66: 846--850.

Peter J. Rousseeuw (1987). "Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis". Computational and Applied Mathematics. 20: 53–65. doi:10.1016/0377-0427(87)90125-7.

Sharma, S.C. (1996). Applied Multivariate Techniques. John Wiley and Sons.

Sørensen, T. (1948). "A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on Danish commons". Kongelige Danske Videnskabernes Selskab. 5 (4): 1–34.

Theodoridis, S. and Koutroubas, K. (1999). Pattern Recognition. Academic Press.
